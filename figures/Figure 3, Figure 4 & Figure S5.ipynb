{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: load all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import YearLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Arial'\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from itertools import groupby\n",
    "from scipy import integrate\n",
    "from tqdm import tqdm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from functions import sum_tuples,calc_risk_total,set_prot_standard,pluvial_design,pluvial_design_rail,get_value,get_mean,wbregion\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('chained_assignment',None)\n",
    "\n",
    "sys.path.append(os.path.join( '..'))\n",
    "from miriam_py.utils import load_config\n",
    "data_path = load_config()['paths']['data']\n",
    "figure_path = load_config()['paths']['figures']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load datasets with additional information about countries and regions to create the maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "global_countries = gpd.read_file(os.path.join(data_path,'input_data','global_countries.shp'))\n",
    "global_regions = gpd.read_file(os.path.join(data_path,'input_data','global_regions_v2.shp'))\n",
    "prot_lookup = dict(zip(global_regions['GID_2'],global_regions['prot_stand']))\n",
    "ne_countries = gpd.read_file(os.path.join(data_path,'input_data','ne_50m_admin_0_countries.shp'))\n",
    "ne_sindex = ne_countries.sindex\n",
    "\n",
    "incomegroups = pd.read_csv(os.path.join(data_path,'input_data','incomegroups_2018.csv'),index_col=[0])\n",
    "global_countries['GDP'] = global_countries.geometry.apply(lambda x: get_value(x,ne_sindex,ne_countries,'GDP_MD_EST'))\n",
    "GDP_group = global_countries.merge(incomegroups,left_on='ISO_3digit',right_on='CountryCode').groupby('GroupName')['GDP'].sum()\n",
    "GDP_group = pd.DataFrame(GDP_group)\n",
    "GDP_group = GDP_group*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbccodes = pd.read_csv(os.path.join(data_path,'input_data','wbccodes2014.csv'),index_col=[0])\n",
    "wbc_lookup = dict(zip(wbccodes.index,wbccodes.wbregion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Specify which assets we would like to remove from railway dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail_to_remove =['disused','abandoned','dismantled','preserved', 'proposed','razed', \n",
    "                 'planned','no','historical','na','not_built','abandonned', 'uncompleted', 'demolished',\n",
    "                 'abandoned_tram','construction;rail', 'rail;construction','waste_disposal','collapsed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load all damage estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "events_FU = ['FU-5', 'FU-10', 'FU-20', 'FU-50', 'FU-75', 'FU-100', 'FU-200', 'FU-250','FU-500', 'FU-1000']\n",
    "tot_road_FU = pd.read_csv(os.path.join(data_path,'summarized','FU_road_losses.csv'),\n",
    "                          converters = dict(zip(events_FU,[ast.literal_eval]*len(events_FU))),index_col=[0]) \n",
    "print('FU loaded')\n",
    "\n",
    "events_PU = ['PU-5', 'PU-10', 'PU-20', 'PU-50', 'PU-75', 'PU-100', 'PU-200', 'PU-250', 'PU-500', 'PU-1000']\n",
    "tot_road_PU = pd.read_csv(os.path.join(data_path,'summarized','PU_road_losses.csv'),\n",
    "                          converters = dict(zip(events_PU,[ast.literal_eval]*len(events_PU))),index_col=[0]) \n",
    "print('PU loaded')\n",
    "\n",
    "events_EQ = ['EQ_rp250','EQ_rp475','EQ_rp975','EQ_rp1500','EQ_rp2475'] \n",
    "tot_road_EQ = pd.read_csv(os.path.join(data_path,'summarized','EQ_road_losses.csv'),\n",
    "                          converters = dict(zip(events_EQ,[ast.literal_eval]*len(events_EQ))),index_col=[0]) \n",
    "print('EQ loaded')\n",
    "\n",
    "events_Cyc = ['Cyc_rp50','Cyc_rp100','Cyc_rp250','Cyc_rp500','Cyc_rp1000']\n",
    "tot_road_Cyc = pd.read_csv(os.path.join(data_path,'summarized','Cyc_road_losses_uncer.csv'),\n",
    "                           converters = dict(zip(events_Cyc,[ast.literal_eval]*len(events_Cyc))))\n",
    "print('Cyc loaded')\n",
    "\n",
    "events_CF = ['CF-10', 'CF-20', 'CF-50', 'CF-100', 'CF-200', 'CF-500', 'CF-1000']\n",
    "tot_road_CF = pd.read_csv(os.path.join(data_path,'summarized','CF_road_losses.csv'),\n",
    "                          converters = dict(zip(events_CF,[ast.literal_eval]*len(events_CF))),index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "events_FU = ['FU-5', 'FU-10', 'FU-20', 'FU-50', 'FU-75', 'FU-100', 'FU-200', 'FU-250','FU-500', 'FU-1000']\n",
    "tot_rail_FU = pd.read_csv(os.path.join(data_path,'summarized','FU_rail_losses.csv'),\n",
    "                          converters = dict(zip(events_FU,[ast.literal_eval]*len(events_FU))),index_col=[0]) \n",
    "print('FU loaded')\n",
    "\n",
    "events_PU = ['PU-5', 'PU-10', 'PU-20', 'PU-50', 'PU-75', 'PU-100', 'PU-200', 'PU-250', 'PU-500', 'PU-1000']\n",
    "tot_rail_PU = pd.read_csv(os.path.join(data_path,'summarized','PU_rail_losses.csv'),\n",
    "                          converters = dict(zip(events_PU,[ast.literal_eval]*len(events_PU))),index_col=[0]) \n",
    "print('PU loaded')\n",
    "\n",
    "events_EQ = ['EQ_rp250','EQ_rp475','EQ_rp975','EQ_rp1500','EQ_rp2475'] \n",
    "tot_rail_EQ = pd.read_csv(os.path.join(data_path,'summarized','EQ_rail_losses.csv'),\n",
    "                          converters = dict(zip(events_EQ,[ast.literal_eval]*len(events_EQ))),index_col=[0]) \n",
    "print('EQ loaded')\n",
    "\n",
    "events_Cyc = ['Cyc_rp50','Cyc_rp100','Cyc_rp250','Cyc_rp500','Cyc_rp1000']\n",
    "tot_rail_Cyc = pd.read_csv(os.path.join(data_path,'summarized','Cyc_rail_losses.csv'),\n",
    "                           converters = dict(zip(events_Cyc,[ast.literal_eval]*len(events_Cyc))))\n",
    "print('Cyc loaded')\n",
    "\n",
    "events_CF = ['CF-10', 'CF-20', 'CF-50', 'CF-100', 'CF-200', 'CF-500', 'CF-1000']\n",
    "tot_rail_CF = pd.read_csv(os.path.join(data_path,'summarized','CF_rail_losses.csv'),\n",
    "                          converters = dict(zip(events_CF,[ast.literal_eval]*len(events_CF))),index_col=[0])\n",
    "\n",
    "print('CF loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tot_bridge_road = pd.read_csv(os.path.join(data_path,'summarized','bridge_road_risk_.csv'),index_col=[0],\n",
    "                              converters = dict(zip(['CF_risk','Cyc_risk','EQ_risk','FU_risk','PU_risk'],[ast.literal_eval]*5)))\n",
    "\n",
    "tot_bridge_road.IncomeGroup = tot_bridge_road.IncomeGroup.apply(lambda x :x.upper())\n",
    "tot_bridge_rail = pd.read_csv(os.path.join(data_path,'summarized','bridge_rail_risk_.csv'),\n",
    "                          converters = dict(zip(['CF_risk','Cyc_risk','EQ_risk','FU_risk','PU_risk'],[ast.literal_eval]*len(['CF_risk','Cyc_risk','EQ_risk','FU_risk','PU_risk']))),index_col=[0])\n",
    "\n",
    "tot_bridge_rail.IncomeGroup = tot_bridge_rail.IncomeGroup.apply(lambda x :x.upper())\n",
    "\n",
    "wb_road_bridge = tot_bridge_road.loc[tot_bridge_road.road_type.isin(['primary','secondary','tertiary'])].groupby(\n",
    "    ['IncomeGroup'])['CF_risk','Cyc_risk','EQ_risk','FU_risk','PU_risk'].agg(sum_tuples)\n",
    "\n",
    "wb_rail_bridge = tot_bridge_rail.groupby(['IncomeGroup'])['CF_risk','Cyc_risk','EQ_risk','FU_risk','PU_risk'].agg(sum_tuples)\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Add additional information to the output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_road_FU = tot_road_FU.merge(incomegroups,left_on='country',right_on='CountryCode').merge(global_countries[['ISO_3digit','wbregion']],left_on='country',right_on='ISO_3digit')\n",
    "tot_road_PU = tot_road_PU.merge(incomegroups,left_on='country',right_on='CountryCode').merge(global_countries[['ISO_3digit','wbregion']],left_on='country',right_on='ISO_3digit')\n",
    "tot_road_Cyc = tot_road_Cyc.merge(incomegroups,left_on='country',right_on='CountryCode')\n",
    "tot_road_EQ = tot_road_EQ.merge(incomegroups,left_on='country',right_on='CountryCode')\n",
    "tot_road_CF = tot_road_CF.merge(incomegroups,left_on='country',right_on='CountryCode')\n",
    "\n",
    "tot_rail_FU = tot_rail_FU.merge(incomegroups,left_on='country',right_on='CountryCode').merge(global_countries[['ISO_3digit','wbregion']],left_on='country',right_on='ISO_3digit')\n",
    "tot_rail_PU = tot_rail_PU.merge(incomegroups,left_on='country',right_on='CountryCode').merge(global_countries[['ISO_3digit','wbregion']],left_on='country',right_on='ISO_3digit')\n",
    "tot_rail_Cyc = tot_rail_Cyc.merge(incomegroups,left_on='country',right_on='CountryCode')\n",
    "tot_rail_EQ = tot_rail_EQ.merge(incomegroups,left_on='country',right_on='CountryCode')\n",
    "tot_rail_CF = tot_rail_CF.merge(incomegroups,left_on='country',right_on='CountryCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tqdm.pandas()\n",
    "tot_road_FU = tot_road_FU.progress_apply(lambda x : set_prot_standard(x,prot_lookup,events_FU),axis=1)\n",
    "tot_road_CF = tot_road_CF.progress_apply(lambda x : set_prot_standard(x,prot_lookup,events_CF),axis=1)\n",
    "\n",
    "tot_rail_FU = tot_rail_FU.progress_apply(lambda x : set_prot_standard(x,prot_lookup,events_FU),axis=1)\n",
    "tot_rail_CF = tot_rail_CF.progress_apply(lambda x : set_prot_standard(x,prot_lookup,events_CF),axis=1)\n",
    "\n",
    "tot_road_FU = tot_road_FU.progress_apply(lambda x : pluvial_design(x,'FU'),axis=1)\n",
    "tot_road_CF = tot_road_CF.progress_apply(lambda x : pluvial_design(x,'CF'),axis=1)\n",
    "tot_road_PU = tot_road_PU.progress_apply(lambda x : pluvial_design(x,'PU'),axis=1)\n",
    "\n",
    "tot_rail_FU = tot_rail_FU.progress_apply(lambda x : pluvial_design_rail(x,'FU'),axis=1)\n",
    "tot_rail_CF = tot_rail_CF.progress_apply(lambda x : pluvial_design_rail(x,'CF'),axis=1)\n",
    "tot_rail_PU = tot_rail_PU.progress_apply(lambda x : pluvial_design_rail(x,'PU'),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Group datasets to the four income levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FU_wb_stats_road = tot_road_FU.loc[tot_road_FU.road_type.isin(['primary','secondary','tertiary'])].groupby(\n",
    "    ['road_type','GroupName'])[events_FU].agg(sum_tuples)\n",
    "PU_wb_stats_road = tot_road_PU.loc[tot_road_PU.road_type.isin(['primary','secondary','tertiary'])].groupby(\n",
    "    ['road_type','GroupName'])[events_PU].agg(sum_tuples)\n",
    "Cyc_wb_stats_road = tot_road_Cyc.loc[tot_road_Cyc.road_type.isin(['primary','secondary','tertiary'])].groupby(\n",
    "    ['road_type','GroupName'])[events_Cyc].agg(sum_tuples)\n",
    "EQ_wb_stats_road = tot_road_EQ.loc[tot_road_EQ.road_type.isin(['primary','secondary','tertiary'])].groupby(\n",
    "    ['road_type','GroupName'])[events_EQ].agg(sum_tuples)\n",
    "CF_wb_stats_road = tot_road_CF.loc[tot_road_CF.road_type.isin(['primary','secondary','tertiary'])].groupby(\n",
    "    ['road_type','GroupName'])[events_CF].agg(sum_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FU_wb_stats_rail = tot_rail_FU.loc[~(tot_rail_FU.infra_type.isin(rail_to_remove))].groupby(\n",
    "    ['GroupName'])[events_FU].agg(sum_tuples)\n",
    "PU_wb_stats_rail = tot_rail_PU.loc[~(tot_rail_PU.infra_type.isin(rail_to_remove))].groupby(\n",
    "                ['GroupName'])[events_PU].agg(sum_tuples)\n",
    "Cyc_wb_stats_rail = tot_rail_Cyc.loc[~(tot_rail_Cyc.infra_type.isin(rail_to_remove))].groupby(\n",
    "    ['GroupName'])[events_Cyc].agg(sum_tuples)\n",
    "EQ_wb_stats_rail = tot_rail_EQ.loc[~(tot_rail_EQ.infra_type.isin(rail_to_remove))].groupby(\n",
    "    ['GroupName'])[events_EQ].agg(sum_tuples)\n",
    "CF_wb_stats_rail = tot_rail_CF.loc[~(tot_rail_CF.infra_type.isin(rail_to_remove))].groupby(\n",
    "    ['GroupName'])[events_CF].agg(sum_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_roads = pd.read_feather(os.path.join(data_path,'summarized','all_road_stats.ft'))\n",
    "tot_roads = tot_roads.loc[tot_roads.road_type.isin(['primary','secondary','tertiary'])]\n",
    "tot_roads = tot_roads.merge(global_countries[['ISO_3digit','wbincomena']],left_on='country',right_on='ISO_3digit')\n",
    "tot_roads = tot_roads.merge(incomegroups,left_on='country',right_on='CountryCode')\n",
    "tot_len_road = pd.DataFrame(tot_roads.groupby('GroupName')['length'].sum(),columns=['length'])\n",
    "lookup_length_road = dict(zip(tot_roads.index,tot_roads.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_rail = pd.read_feather(os.path.join(data_path,'summarized','all_railway_stats.ft'))\n",
    "tot_rail = tot_rail.loc[~(tot_rail.infra_type.isin(rail_to_remove))]\n",
    "\n",
    "tot_rail = tot_rail.merge(global_countries[['ISO_3digit','wbincomena']],left_on='country',right_on='ISO_3digit')\n",
    "tot_rail = tot_rail.merge(incomegroups,left_on='country',right_on='CountryCode')\n",
    "\n",
    "tot_len_rail = pd.DataFrame(tot_rail.groupby('GroupName')['length'].sum(),columns=['length'])\n",
    "\n",
    "tot_len = tot_len_road.add(tot_len_rail, fill_value=0)\n",
    "lookup_length = dict(zip(tot_len.index,tot_len.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_infra_value = pd.read_csv(os.path.join(data_path,'summarized','total_infrastructure_values.csv'))\n",
    "total_infra_value = total_infra_value.merge(global_countries[['ISO_3digit','wbincomena']],left_on='country',right_on='ISO_3digit')\n",
    "total_infra_value = total_infra_value.merge(incomegroups,left_on='country',right_on='CountryCode')\n",
    "wbinc_infra_value = pd.DataFrame(total_infra_value.groupby('GroupName')['infra_value'].sum(),columns=['infra_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### absolute numbers\n",
    "hazards = ['Cyc','EQ','PU','FU','CF','Tot','Cyc','EQ','PU','FU','CF','Tot']\n",
    "hazards_full = ['Cyclones','Earthquakes','Surface Flooding','River Flooding','Coastal Flooding','Total Risk','Cyclones','Earthquakes','Surface Flooding','River Flooding','Coastal Flooding','Total']\n",
    "fig, ax_losses = plt.subplots(2, 6,figsize=(25,14), sharex='col')  \n",
    "gdp_income = dict(zip(list(GDP_group.index),list(GDP_group.GDP)))\n",
    "\n",
    "color_scheme = ['#F3FFBD','#B2DBBF','#70C1B3','#247BA0'] \n",
    "income_dict = ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "income_axis = ['Low\\nincome', 'Lower middle\\nincome', 'Upper middle\\nincome', 'High\\nincome']\n",
    "\n",
    "color_lookup = dict(zip(income_dict,color_scheme))\n",
    "axis_lookup = dict(zip(income_dict,income_axis))\n",
    "\n",
    "wb_agg = []\n",
    "wb_agg_sum = []\n",
    "wb_rel = {}\n",
    "for iter_,ax_loss in enumerate(ax_losses.flat):\n",
    "    if iter_ == 6:\n",
    "        wb_agg = []\n",
    "    if (iter_ == 0) | (iter_ == 6):\n",
    "        RPS = [1/50,1/100,1/250,1/500,1/1000]\n",
    "        wb_risk_road = pd.DataFrame(Cyc_wb_stats_road.apply(lambda x: calc_risk_total(x,'Cyc',RPS,events_Cyc),axis=1).tolist(),\n",
    "                               index=Cyc_wb_stats_road.index,\n",
    "             columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "        wb_risk_rail = pd.DataFrame(Cyc_wb_stats_rail.apply(lambda x: calc_risk_total(x,'Cyc',RPS,events_Cyc),axis=1).tolist(),\n",
    "                               index=Cyc_wb_stats_rail.index,\n",
    "             columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "        \n",
    "        wb_br_ro = pd.DataFrame(wb_road_bridge['Cyc_risk'].apply(pd.Series))\n",
    "        wb_br_ro.columns = ['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100']\n",
    "        wb_br_ro = pd.concat([wb_br_ro.iloc[1:,:], pd.DataFrame(wb_br_ro.iloc[0,:]).T])\n",
    "        wb_br_ro.index =  ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "\n",
    "        wb_br_ra = pd.DataFrame(wb_rail_bridge['Cyc_risk'].apply(pd.Series))\n",
    "        wb_br_ra.columns = ['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100']\n",
    "        wb_br_ra = pd.concat([wb_br_ra.iloc[1:,:], pd.DataFrame(wb_br_ra.iloc[0,:]).T])\n",
    "        wb_br_ra.index =  ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "\n",
    "\n",
    "        wb_rel['Cyc'] = pd.concat(dict(zip(['Primary Roads','Secondary Roads','Tertiary Roads','Rail','Bridge_Road','Bridge_Rail'],[wb_risk_road.loc['primary'],\n",
    "                                                                                       wb_risk_road.loc['secondary'],wb_risk_road.loc['tertiary'],\n",
    "                                                                                       wb_risk_rail,wb_br_ro,wb_br_ra])))\n",
    "        wb_risk = wb_risk_road.groupby('GroupName').sum().add(wb_risk_rail).add(wb_br_ro).add(wb_br_ra)        \n",
    "        wb_agg.append(wb_risk)\n",
    "        \n",
    "    elif (iter_ == 1) | (iter_ == 7):\n",
    "        RPS = [1/250,1/475,1/975,1/1500,1/2475]\n",
    "        wb_risk_road = pd.DataFrame(EQ_wb_stats_road.apply(lambda x: calc_risk_total(x,'EQ',RPS,events_EQ),axis=1).tolist(),\n",
    "                               index=EQ_wb_stats_road.index,\n",
    "             columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "        wb_risk_rail = pd.DataFrame(EQ_wb_stats_rail.apply(lambda x: calc_risk_total(x,'EQ',RPS,events_EQ),axis=1).tolist(),\n",
    "                               index=EQ_wb_stats_rail.index,\n",
    "             columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])        \n",
    "        wb_br_ro = pd.DataFrame(wb_road_bridge['EQ_risk'].apply(pd.Series))\n",
    "        wb_br_ro.columns = ['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100']\n",
    "        wb_br_ro = pd.concat([wb_br_ro.iloc[1:,:], pd.DataFrame(wb_br_ro.iloc[0,:]).T])\n",
    "        wb_br_ro.index =  ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "\n",
    "        wb_br_ra = pd.DataFrame(wb_rail_bridge['EQ_risk'].apply(pd.Series))\n",
    "        wb_br_ra.columns = ['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100']\n",
    "        wb_br_ra = pd.concat([wb_br_ra.iloc[1:,:], pd.DataFrame(wb_br_ra.iloc[0,:]).T])\n",
    "        wb_br_ra.index =  ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "\n",
    "        wb_rel['EQ'] = pd.concat(dict(zip(['Primary Roads','Secondary Roads','Tertiary Roads','Rail','Bridge_Road','Bridge_Rail'],[wb_risk_road.loc['primary'],\n",
    "                                                                                       wb_risk_road.loc['secondary'],wb_risk_road.loc['tertiary'],\n",
    "                                                                                       wb_risk_rail,wb_br_ro,wb_br_ra])))\n",
    "        wb_risk = wb_risk_road.groupby('GroupName').sum().add(wb_risk_rail).add(wb_br_ro).add(wb_br_ra)        \n",
    "        wb_agg.append(wb_risk)\n",
    "    elif (iter_ == 2) | (iter_ == 8):\n",
    "        RPS = [1/5,1/10,1/20,1/50,1/75,1/100,1/200,1/250,1/500,1/1000]\n",
    "        wb_risk_road = pd.DataFrame(PU_wb_stats_road.apply(lambda x: calc_risk_total(x,'PU',RPS,events_PU),axis=1).tolist(),\n",
    "                               index=PU_wb_stats_road.index,\n",
    "             columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "        wb_risk_rail = pd.DataFrame(PU_wb_stats_rail.apply(lambda x: calc_risk_total(x,'PU',RPS,events_PU),axis=1).tolist(),\n",
    "                               index=PU_wb_stats_rail.index,\n",
    "             columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])        \n",
    "        wb_br_ro = pd.DataFrame(wb_road_bridge['PU_risk'].apply(pd.Series))\n",
    "        wb_br_ro.columns = ['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100']\n",
    "        wb_br_ro = pd.concat([wb_br_ro.iloc[1:,:], pd.DataFrame(wb_br_ro.iloc[0,:]).T])\n",
    "        wb_br_ro.index =  ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "\n",
    "        wb_br_ra = pd.DataFrame(wb_rail_bridge['PU_risk'].apply(pd.Series))\n",
    "        wb_br_ra.columns = ['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100']\n",
    "        wb_br_ra = pd.concat([wb_br_ra.iloc[1:,:], pd.DataFrame(wb_br_ra.iloc[0,:]).T])\n",
    "        wb_br_ra.index =  ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "\n",
    "        wb_rel['PU'] = pd.concat(dict(zip(['Primary Roads','Secondary Roads','Tertiary Roads','Rail','Bridge_Road','Bridge_Rail'],[wb_risk_road.loc['primary'],\n",
    "                                                                                       wb_risk_road.loc['secondary'],wb_risk_road.loc['tertiary'],\n",
    "                                                                                       wb_risk_rail,wb_br_ro,wb_br_ra])))\n",
    "        wb_risk = wb_risk_road.groupby('GroupName').sum().add(wb_risk_rail).add(wb_br_ro).add(wb_br_ra)        \n",
    "        wb_agg.append(wb_risk) \n",
    "        \n",
    "    elif (iter_ == 3) | (iter_ == 9):\n",
    "        RPS = [1/5,1/10,1/20,1/50,1/75,1/100,1/200,1/250,1/500,1/1000]\n",
    "        wb_risk_road = pd.DataFrame(FU_wb_stats_road.apply(lambda x: calc_risk_total(x,'FU',RPS,events_FU),axis=1).tolist(),\n",
    "                               index=FU_wb_stats_road.index,\n",
    "             columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "        wb_risk_rail = pd.DataFrame(FU_wb_stats_rail.apply(lambda x: calc_risk_total(x,'FU',RPS,events_FU),axis=1).tolist(),\n",
    "                               index=FU_wb_stats_rail.index,\n",
    "             columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])        \n",
    "        wb_br_ro = pd.DataFrame(wb_road_bridge['FU_risk'].apply(pd.Series))\n",
    "        wb_br_ro.columns = ['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100']\n",
    "        wb_br_ro = pd.concat([wb_br_ro.iloc[1:,:], pd.DataFrame(wb_br_ro.iloc[0,:]).T])\n",
    "        wb_br_ro.index =  ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "\n",
    "        wb_br_ra = pd.DataFrame(wb_rail_bridge['FU_risk'].apply(pd.Series))\n",
    "        wb_br_ra.columns = ['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100']\n",
    "        wb_br_ra = pd.concat([wb_br_ra.iloc[1:,:], pd.DataFrame(wb_br_ra.iloc[0,:]).T])\n",
    "        wb_br_ra.index =  ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "\n",
    "        wb_rel['FU'] = pd.concat(dict(zip(['Primary Roads','Secondary Roads','Tertiary Roads','Rail','Bridge_Road','Bridge_Rail'],[wb_risk_road.loc['primary'],\n",
    "                                                                                       wb_risk_road.loc['secondary'],wb_risk_road.loc['tertiary'],\n",
    "                                                                                       wb_risk_rail,wb_br_ro,wb_br_ra])))\n",
    "        wb_risk = wb_risk_road.groupby('GroupName').sum().add(wb_risk_rail).add(wb_br_ro).add(wb_br_ra)        \n",
    "        wb_agg.append(wb_risk) \n",
    "\n",
    "    elif (iter_ == 4) | (iter_ == 10):\n",
    "        RPS = [1/10,1/20,1/50,1/100,1/200,1/500,1/1000]\n",
    "        wb_risk_road = pd.DataFrame(CF_wb_stats_road.apply(lambda x: calc_risk_total(x,'CF',RPS,events_CF),axis=1).tolist(),\n",
    "                               index=CF_wb_stats_road.index,\n",
    "             columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "        wb_risk_rail = pd.DataFrame(CF_wb_stats_rail.apply(lambda x: calc_risk_total(x,'CF',RPS,events_CF),axis=1).tolist(),\n",
    "                               index=CF_wb_stats_rail.index,\n",
    "             columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])        \n",
    "        wb_br_ro = pd.DataFrame(wb_road_bridge['CF_risk'].apply(pd.Series))\n",
    "        wb_br_ro.columns = ['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100']\n",
    "        wb_br_ro = pd.concat([wb_br_ro.iloc[1:,:], pd.DataFrame(wb_br_ro.iloc[0,:]).T])\n",
    "        wb_br_ro.index =  ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "\n",
    "        wb_br_ra = pd.DataFrame(wb_rail_bridge['CF_risk'].apply(pd.Series))\n",
    "        wb_br_ra.columns = ['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100']\n",
    "        wb_br_ra = pd.concat([wb_br_ra.iloc[1:,:], pd.DataFrame(wb_br_ra.iloc[0,:]).T])\n",
    "        wb_br_ra.index =  ['Low income', 'Lower middle income', 'Upper middle income', 'High income']\n",
    "\n",
    "        wb_rel['CF'] = pd.concat(dict(zip(['Primary Roads','Secondary Roads','Tertiary Roads','Rail','Bridge_Road','Bridge_Rail'],[wb_risk_road.loc['primary'],\n",
    "                                                                                       wb_risk_road.loc['secondary'],wb_risk_road.loc['tertiary'],\n",
    "                                                                                       wb_risk_rail,wb_br_ro,wb_br_ra])))\n",
    "        wb_risk = wb_risk_road.groupby('GroupName').sum().add(wb_risk_rail).add(wb_br_ro).add(wb_br_ra)        \n",
    "        wb_agg.append(wb_risk) \n",
    "\n",
    "    elif (iter_ == 5) | (iter_ == 11):\n",
    "        wb_risk = pd.concat(wb_agg).groupby(level=0,axis=0).sum()\n",
    "        \n",
    "    if iter_ < 6:\n",
    "        wb_agg_sum.append(wb_risk)\n",
    "        wb_risk = wb_risk.T\n",
    "        wb_risk = pd.DataFrame(wb_risk.unstack(0),columns=['data'])\n",
    "        wb_risk = wb_risk/1000000000\n",
    "        wb_risk.index.names = ['wbincome','perc']\n",
    "        wb_risk.reset_index(inplace=True)\n",
    "        wb_risk.wbincome = wb_risk.wbincome.astype(\"category\")\n",
    "        wb_risk.wbincome.cat.set_categories(income_dict, inplace=True)\n",
    "        wb_risk = wb_risk.sort_values([\"wbincome\"])\n",
    "        sorted_income = list(wb_risk.wbincome.unique())\n",
    "        wb_risk['wbincome'] = wb_risk.wbincome.apply(lambda x: axis_lookup[x])\n",
    "\n",
    "        sns.boxplot(x=\"wbincome\", y=\"data\",\n",
    "                                 data=wb_risk,showfliers=False,ax=ax_loss)   \n",
    "    elif iter_ > 5:\n",
    "        wb_risk = wb_risk.T\n",
    "        if iter_ != 11:\n",
    "            for col in wb_risk.columns:\n",
    "                wb_risk[col] = wb_risk[col]/lookup_length[col]\n",
    "\n",
    "        wb_risk = pd.DataFrame(wb_risk.unstack(0),columns=['data'])\n",
    "        wb_risk.index.names = ['wbincome','perc']\n",
    "        wb_risk.reset_index(inplace=True)\n",
    "        wb_risk.wbincome = wb_risk.wbincome.astype(\"category\")\n",
    "        wb_risk.wbincome.cat.set_categories(income_dict, inplace=True)\n",
    "        wb_risk = wb_risk.sort_values([\"wbincome\"])\n",
    "\n",
    "        sorted_income = list(wb_risk.wbincome.unique())\n",
    "        wb_risk['wbincome'] = wb_risk.wbincome.apply(lambda x: axis_lookup[x])\n",
    "        sns.boxplot(x=\"wbincome\", y=\"data\",\n",
    "                                 data=wb_risk,showfliers=False,ax=ax_loss)   \n",
    "\n",
    "    if iter_ > 5:\n",
    "        for tick in ax_loss.get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "        ax_loss.set_xlabel('Income Group', fontsize=24,fontweight='bold',color='black')\n",
    "        ax_loss.get_xaxis().set_label_coords(0.5,-0.3)\n",
    "    else:\n",
    "        ax_loss.set_title(hazards_full[iter_],fontweight='bold', fontsize=22)\n",
    "        ax_loss.set_xlabel('')\n",
    "        \n",
    "    for y in range(4):\n",
    "        ax_loss.findobj(matplotlib.patches.Patch)[y].set_facecolor(color_lookup[sorted_income[y]])\n",
    "        ax_loss.findobj(matplotlib.patches.Patch)[y].set_edgecolor('black')\n",
    "\n",
    "    ax_loss.set_facecolor('#FAF9F9')\n",
    "    ax_loss.spines['left'].set_color('black')\n",
    "    ax_loss.spines['bottom'].set_color('black')\n",
    "    ax_loss.tick_params(axis = 'both',labelsize=20,labelcolor='black',color='black')\n",
    "\n",
    "    yaxis_allticks = [np.arange(0,0.61,0.15),np.arange(0,1.1,0.2),np.arange(0,4.1,0.8),np.arange(0,5.1,1),np.arange(0,2.01,0.4),np.arange(0,12.1,2)\n",
    "                      ,np.arange(0,31,7.5),np.arange(0,61,10),np.arange(0,401,80),np.arange(0,351,70),np.arange(0,201,40),np.arange(0,801,200)]\n",
    "\n",
    "    ax_loss.yaxis.set_ticks(yaxis_allticks[iter_])\n",
    "    ax_loss.set_ylim([yaxis_allticks[iter_][0],yaxis_allticks[iter_][-1]])\n",
    "    ax_loss.set_facecolor('#FAF9F9')\n",
    "    if iter_ == 0:\n",
    "        ax_loss.set_ylabel('EAD in billion USD', fontsize=24,fontweight='bold',color='black')\n",
    "        ax_loss.get_yaxis().set_label_coords(-0.3,0.5)\n",
    "    elif iter_ == 6:\n",
    "        ax_loss.set_ylabel('EAD per km in USD', fontsize=24,fontweight='bold',color='black')\n",
    "        ax_loss.get_yaxis().set_label_coords(-0.3,0.5)\n",
    "    else:\n",
    "        ax_loss.set_ylabel('')\n",
    "        \n",
    "\n",
    "fig.subplots_adjust(wspace=0.05, hspace=0.1,top=0.88)\n",
    "fig.tight_layout(rect=[0, 0, 1, 1])    \n",
    "fig.savefig(os.path.join(figure_path,'Fig4_losses.png'),dpi=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hzd_sum = pd.concat(dict(zip(['Cyc','EQ','PU','FU','CF'],wb_agg_sum))).groupby(level=0,axis=0).sum()/1e9\n",
    "hzd_sum = hzd_sum.reindex(['Cyc','EQ','CF','FU','PU'])\n",
    "hzd_sum.index = ['Cyclones','Earth-\\nquakes','Coastal\\nFlooding','River\\nFlooding','Surface\\nFlooding']\n",
    "data = pd.DataFrame(hzd_sum.stack(),columns=['value']).reset_index()\n",
    "\n",
    "fig  = plt.figure(figsize=(20,7)) #, sharey='row'\n",
    "gs = gridspec.GridSpec(1, 12)\n",
    "\n",
    "letters = ['A','B','C']\n",
    "\n",
    "color_scheme_map = ['#264653','#2A9D8F','#E9C46A','#DB9157','#E76F51'] \n",
    "\n",
    "for iter_ in range(3):\n",
    "    if iter_ == 0:\n",
    "        df = data.loc[data.level_1 == 'perc_50']\n",
    "        df = df.groupby(['level_0','level_1']).sum()\n",
    "        df.columns=['val1']\n",
    "        df = df.reindex(['Cyclones','Earth-\\nquakes','Coastal\\nFlooding','River\\nFlooding','Surface\\nFlooding'],level=0)\n",
    "        df['cum_sum'] = df.val1.cumsum()\n",
    "        df['cum_perc'] = df.cum_sum/df.val1.sum()\n",
    "        df['theta'] = df.cum_perc*360\n",
    "        df['perc'] = df.val1/df.val1.sum()*100\n",
    "        df['perc'] = df['perc'].apply(lambda x : round(x,1))\n",
    "        thetas = list(df['theta'])        \n",
    "        ax = plt.subplot(gs[0,:4])\n",
    "        \n",
    "        wed1 = matplotlib.patches.Wedge((0.5, 0.5), 0.3,0, thetas[0],facecolor=color_scheme_map[0],linewidth=1,edgecolor='white')\n",
    "        wed2 = matplotlib.patches.Wedge((0.5, 0.5), 0.3,thetas[0], thetas[1],facecolor=color_scheme_map[1],linewidth=1,edgecolor='white')\n",
    "        wed3 = matplotlib.patches.Wedge((0.5, 0.5), 0.3,thetas[1], thetas[2],facecolor=color_scheme_map[2],linewidth=1,edgecolor='white')\n",
    "        wed4 = matplotlib.patches.Wedge((0.5, 0.5), 0.3,thetas[2], thetas[3],facecolor=color_scheme_map[3],linewidth=1,edgecolor='white')\n",
    "        wed5 = matplotlib.patches.Wedge((0.5, 0.5), 0.3,thetas[3], thetas[4],facecolor=color_scheme_map[4],linewidth=1,edgecolor='white')\n",
    "\n",
    "        # set translation \n",
    "        ax.add_patch(wed1)\n",
    "        ax.add_patch(wed2)\n",
    "        ax.add_patch(wed3)\n",
    "        ax.add_patch(wed4)\n",
    "        ax.add_patch(wed5)\n",
    "        \n",
    "        ax.set_facecolor('white')\n",
    "        ax.set_axis_off()\n",
    "        ax.annotate(\"{}%\".format(df['perc'].iloc[0]), xy=(0.82,0.52), fontsize=18,fontweight='bold')\n",
    "        ax.annotate(\"{}%\".format(df['perc'].iloc[1]), xy=(0.78,0.65), fontsize=18,fontweight='bold')\n",
    "        ax.annotate(\"{}%\".format(df['perc'].iloc[2]), xy=(0.6,0.8), fontsize=18,fontweight='bold')\n",
    "        ax.annotate(\"{}%\".format(df['perc'].iloc[3]), xy=(0.11,0.66), fontsize=18,fontweight='bold')\n",
    "        ax.annotate(\"{}%\".format(df['perc'].iloc[4]), xy=(0.58,0.15), fontsize=18,fontweight='bold')\n",
    "        ax.text(0, 0.99, '{})'.format(letters[iter_]), transform=ax.transAxes,fontweight=\"bold\",color='black', fontsize=20,\n",
    "            verticalalignment='top', bbox= dict(boxstyle='round', facecolor='white', alpha=0.5,linewidth=0))   \n",
    "    if iter_ == 1:\n",
    "        data2 = pd.DataFrame(hzd_sum.sum(axis=0),columns=['value']).reset_index()\n",
    "        data2['level_0'] = 'Multi\\nHazard'\n",
    "        ax = plt.subplot(gs[0,5:6])\n",
    "\n",
    "\n",
    "        sns.boxplot(data=data2,x='level_0',y='value',showfliers=False,color='#8D99AE')\n",
    "        ax.set_facecolor('#FAF9F9')\n",
    "        ax.set_ylabel('EAD in billion USD', fontsize=20,fontweight='bold',color='black')\n",
    "\n",
    "        ax.spines['left'].set_color('black')\n",
    "        ax.spines['bottom'].set_color('black')\n",
    "        ax.tick_params(axis = 'both',labelsize=18,labelcolor='black',color='black')\n",
    "       \n",
    "        ax.set_xlabel('', fontsize=20,fontweight='bold',color='black')\n",
    "        ax.yaxis.set_ticks(np.arange(0,25.5,5))\n",
    "        ax.findobj(matplotlib.patches.Patch)[0].set_edgecolor('black')\n",
    "        ax.text(-1.5, 0.99, '{})'.format(letters[iter_]), transform=ax.transAxes,fontweight=\"bold\",color='black', fontsize=20,\n",
    "            verticalalignment='top', bbox= dict(boxstyle='round', facecolor='white', alpha=0.5,linewidth=0))   \n",
    "\n",
    "    if iter_ == 2:\n",
    "        ax = plt.subplot(gs[0,7:])\n",
    "        color_scheme_map = ['#264653','#2A9D8F','#E9C46A','#DB9157','#E76F51'] \n",
    "\n",
    "        sns.boxplot(data=data,x='level_0',y='value',showfliers=False)\n",
    "        ax.set_facecolor('#FAF9F9')\n",
    "\n",
    "        ax.set_ylabel('EAD in billion USD', fontsize=20,fontweight='bold',color='black')\n",
    "        ax.spines['left'].set_color('black')\n",
    "        ax.spines['bottom'].set_color('black')\n",
    "        ax.set_xlabel('', fontsize=20,fontweight='bold',color='black')\n",
    "        ax.tick_params(axis = 'both',labelsize=18,labelcolor='black',color='black')\n",
    "        ax.set_ylim([0,10])\n",
    "        \n",
    "        for y in range(5):\n",
    "            ax.findobj(matplotlib.patches.Patch)[y].set_facecolor(color_scheme_map[y])\n",
    "            ax.findobj(matplotlib.patches.Patch)[y].set_edgecolor('black')\n",
    "\n",
    "        ax.text(-0.2, 0.99, '{})'.format(letters[iter_]), transform=ax.transAxes,fontweight=\"bold\",color='black', fontsize=20,\n",
    "            verticalalignment='top', bbox= dict(boxstyle='round', facecolor='white', alpha=0.5,linewidth=0))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(figure_path,'Fig3_total_losses.png'),dpi=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Create Figure S5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 4,figsize=(13,13), sharex='col', sharey='col')  \n",
    "colors = ['#D62828','#F77F00','#FCBF49','#2F545E','#EAE2B7','#99adb3']# ['#fdb863','#b2abd2','#e66101','#5e3c99']\n",
    "\n",
    "data = pd.concat(wb_rel)['perc_50'].unstack([2,0])\n",
    "data = data.reindex(['Primary Roads', 'Secondary Roads', 'Tertiary Roads','Rail','Bridge_Road','Bridge_Rail'])\n",
    "\n",
    "data.plot(kind='pie',subplots=True,colors = colors, labels=None,\n",
    "          ax=ax, radius=1.2,legend=False) # 'linewidth' : 0.5 , 'edgecolor' : 'black', 'alpha' : 0.8}\n",
    "\n",
    "[axi.tick_params(axis='both', which='both', length=0) for iter_,axi in enumerate(ax.ravel())]\n",
    "[axi.patches[4].set_hatch(\"\\\\\") for iter_,axi in enumerate(ax.ravel())]\n",
    "[axi.patches[5].set_hatch(\"\\\\\") for iter_,axi in enumerate(ax.ravel())]\n",
    "\n",
    "hazards_pie = ['Coastal Flooding','Cyclones','Earthquakes','River Flooding','Surface Flooding']\n",
    "\n",
    "[axi.set_xlabel(xlabel=income_dict[iter_-16],fontweight='bold',fontsize=16,color='black') for iter_,axi in enumerate(ax.ravel()) if iter_ > 15]\n",
    "\n",
    "ylocs_hz = [0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4]\n",
    "ylocs = [0,4,8,12,16]\n",
    "[axi.set_ylabel(ylabel=hazards_pie[ylocs_hz[iter_]],fontweight='bold',fontsize=16,color='black') for iter_,axi in enumerate(ax.ravel()) if iter_ in ylocs]\n",
    "\n",
    "legend_elements = [Patch(facecolor=colors[0],label='Primary Roads'), #,edgecolor='black',linewidth=0.3\n",
    "                   Patch(facecolor=colors[1],label='Secondary Roads'),\n",
    "                   Patch(facecolor=colors[2],label='Tertiary Roads'),\n",
    "                   Patch(facecolor=colors[3],label='Railway'),\n",
    "                   Patch(facecolor=colors[4],label='Road Bridges'),                   \n",
    "                   Patch(facecolor=colors[5],label='Railway Bridges')]        \n",
    "\n",
    "[element.set_hatch(\"\\\\\") for iter_,element in enumerate(legend_elements) if iter_ > 3]\n",
    "    \n",
    "[axi.legend(handles=legend_elements,loc='upper center', bbox_to_anchor=(1.55, 1.), shadow=True, \n",
    "                           fancybox=True,facecolor='#fefdfd',prop={'size':14.5}) for iter_,axi in enumerate(ax.ravel()) if iter_ == 3]\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(left=0.05,right=0.82) \n",
    "fig.savefig(os.path.join(figure_path,'FigS5_distribution.png'),dpi=450)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
