{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: load all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import YearLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Arial'\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from itertools import groupby\n",
    "from scipy import integrate\n",
    "from tqdm import tqdm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('chained_assignment',None)\n",
    "\n",
    "sys.path.append(os.path.join( '..'))\n",
    "from miriam_py.utils import load_config\n",
    "data_path = load_config()['paths']['data']\n",
    "figure_path = load_config()['paths']['figures']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create functions required to make the figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_tuples(l):\n",
    "    return tuple(sum(x) for x in zip(*l))\n",
    "\n",
    "def calc_risk_total(x,hazard,RPS,events):\n",
    "    collect_risks = []\n",
    "    for y in range(7):\n",
    "        collect_risks.append(integrate.simps([x[y] for x in x[events]][::-1], x=RPS[::-1]))\n",
    "    return collect_risks\n",
    "\n",
    "def set_prot_standard(x,prot_lookup,events):\n",
    "    prot_stand = prot_lookup[x.region]\n",
    "    no_floods= [z for z in events if prot_stand > int(z.split('-')[1])]\n",
    "    for no_flood in no_floods:\n",
    "        x[no_flood] = (0,0,0,0,0,0,0)\n",
    "    return x\n",
    "\n",
    "def pluvial_design_1up(x,hazard):\n",
    "    if x.GroupCode == 'HIC':\n",
    "        if x.road_type in ['primary','secondary']:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20','PU-50','PU-75','PU-100']] = [(0,0,0,0,0,0,0)]*6\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20','FU-50','FU-75','FU-100']] = [(0,0,0,0,0,0,0)]*6\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20','CF-50','CF-100']] = [(0,0,0,0,0,0,0)]*4               \n",
    "        else:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20','PU-50']] = [(0,0,0,0,0,0,0)]*4\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20','FU-50']] = [(0,0,0,0,0,0,0)]*4\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20','CF-50']] = [(0,0,0,0,0,0,0)]*3\n",
    "    elif x.GroupCode == 'UMC':\n",
    "        if x.road_type in ['primary','secondary']:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20','PU-50']] = [(0,0,0,0,0,0,0)]*4\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20','FU-50']] = [(0,0,0,0,0,0,0)]*4\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20','CF-50']] = [(0,0,0,0,0,0,0)]*3\n",
    "        else:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20']] = [(0,0,0,0,0,0,0)]*3\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20']] = [(0,0,0,0,0,0,0)]*3\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20']] = [(0,0,0,0,0,0,0)]*2\n",
    "    else:\n",
    "        if x.road_type in ['primary','secondary']:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20']] = [(0,0,0,0,0,0,0)]*3\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20']] = [(0,0,0,0,0,0,0)]*3\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20']] = [(0,0,0,0,0,0,0)]*2\n",
    "        else:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10']] = [(0,0,0,0,0,0,0)]*2\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10']] = [(0,0,0,0,0,0,0)]*2\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10']] = [(0,0,0,0,0,0,0)]*1\n",
    "        \n",
    "    return x\n",
    "\n",
    "def pluvial_design_1up_old(x,hazard):\n",
    "    if x.GroupCode == 'HIC':\n",
    "        if x.road_type in ['primary','secondary']:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20','PU-50','PU-75','PU-100','PU-200']] = [(0,0,0,0,0,0,0)]*7\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20','FU-50','FU-75','FU-100','FU-200']] = [(0,0,0,0,0,0,0)]*7\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20','CF-50','CF-100','CF-200']] = [(0,0,0,0,0,0,0)]*5               \n",
    "        else:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20','PU-50','PU-75','PU-100']] = [(0,0,0,0,0,0,0)]*6\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20','FU-50','FU-75','FU-100']] = [(0,0,0,0,0,0,0)]*6\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20','CF-50','CF-100']] = [(0,0,0,0,0,0,0)]*4\n",
    "    elif x.GroupCode == 'UMC':\n",
    "        if x.road_type in ['primary','secondary']:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20','PU-50','PU-75','PU-100']] = [(0,0,0,0,0,0,0)]*6\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20','FU-50','FU-75','FU-100']] = [(0,0,0,0,0,0,0)]*6\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20','CF-50','CF-100']] = [(0,0,0,0,0,0,0)]*4               \n",
    "        else:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20','PU-50']] = [(0,0,0,0,0,0,0)]*4\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20','FU-50']] = [(0,0,0,0,0,0,0)]*4\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20','CF-50']] = [(0,0,0,0,0,0,0)]*3\n",
    "    else:\n",
    "        if x.road_type in ['primary','secondary']:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20','PU-50']] = [(0,0,0,0,0,0,0)]*4\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20','FU-50']] = [(0,0,0,0,0,0,0)]*4\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20','CF-50']] = [(0,0,0,0,0,0,0)]*3\n",
    "        else:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20']] = [(0,0,0,0,0,0,0)]*3\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20']] = [(0,0,0,0,0,0,0)]*3\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20']] = [(0,0,0,0,0,0,0)]*2\n",
    "    return x\n",
    "\n",
    "def pluvial_design(x,hazard):\n",
    "    if x.GroupCode == 'HIC':\n",
    "        if x.road_type in ['primary','secondary']:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20','PU-50']] = [(0,0,0,0,0,0,0)]*4\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20','FU-50']] = [(0,0,0,0,0,0,0)]*4\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20','CF-50']] = [(0,0,0,0,0,0,0)]*3               \n",
    "        else:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20']] = [(0,0,0,0,0,0,0)]*3\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20']] = [(0,0,0,0,0,0,0)]*3\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20']] = [(0,0,0,0,0,0,0)]*2\n",
    "    elif x.GroupCode == 'UMC':\n",
    "        if x.road_type in ['primary','secondary']:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10','PU-20']] = [(0,0,0,0,0,0,0)]*3\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10','FU-20']] = [(0,0,0,0,0,0,0)]*3\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10','CF-20']] = [(0,0,0,0,0,0,0)]*2\n",
    "        else:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10']] = [(0,0,0,0,0,0,0)]*2\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10']] = [(0,0,0,0,0,0,0)]*2\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10']] = [(0,0,0,0,0,0,0)]*1\n",
    "    else:\n",
    "        if x.road_type in ['primary','secondary']:\n",
    "            if hazard == 'PU':\n",
    "                x[['PU-5','PU-10']] = [(0,0,0,0,0,0,0)]*2\n",
    "            elif hazard == 'FU':\n",
    "                x[['FU-5','FU-10']] = [(0,0,0,0,0,0,0)]*2\n",
    "            elif hazard == 'CF':\n",
    "                x[['CF-10']] = [(0,0,0,0,0,0,0)]*1\n",
    "       \n",
    "    return x\n",
    "\n",
    "def get_value(x,ne_sindex,ne_countries,col):\n",
    "    matches = ne_countries.loc[ne_sindex.intersection(x.centroid.bounds[:2])]\n",
    "    \n",
    "    for match in matches.iterrows():\n",
    "        if match[1].geometry.intersects(x) == True:\n",
    "            return match[1][col]\n",
    "        \n",
    "def wbregion(x):\n",
    "    try:\n",
    "        return wbc_lookup[x]\n",
    "    except:\n",
    "        return 'YHI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load datasets with additional information about countries and regions to create the maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_countries = gpd.read_file(os.path.join(data_path,'input_data','global_countries.shp'))\n",
    "global_regions = gpd.read_file(os.path.join(data_path,'input_data','global_regions_v2.shp'))\n",
    "prot_lookup = dict(zip(global_regions['GID_2'],global_regions['prot_stand']))\n",
    "ne_countries = gpd.read_file(os.path.join(data_path,'input_data','ne_50m_admin_0_countries.shp'))\n",
    "ne_sindex = ne_countries.sindex\n",
    "incomegroups = pd.read_csv(os.path.join(data_path,'input_data','incomegroups_2018.csv'),index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbccodes = pd.read_csv(os.path.join(data_path,'input_data','wbccodes2014.csv'),index_col=[0])\n",
    "wbc_lookup = dict(zip(wbccodes.index,wbccodes.wbregion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load flood damages for all road assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "events_FU = ['FU-5', 'FU-10', 'FU-20', 'FU-50', 'FU-75', 'FU-100', 'FU-200', 'FU-250','FU-500', 'FU-1000']\n",
    "tot_road_FU = pd.read_csv(os.path.join(data_path,'summarized','FU_road_losses.csv'),\n",
    "                          converters = dict(zip(events_FU,[ast.literal_eval]*len(events_FU))),index_col=[0]) \n",
    "print('FU loaded')\n",
    "\n",
    "events_PU = ['PU-5', 'PU-10', 'PU-20', 'PU-50', 'PU-75', 'PU-100', 'PU-200', 'PU-250', 'PU-500', 'PU-1000']\n",
    "tot_road_PU = pd.read_csv(os.path.join(data_path,'summarized','PU_road_losses.csv'),\n",
    "                          converters = dict(zip(events_PU,[ast.literal_eval]*len(events_PU))),index_col=[0]) \n",
    "print('PU loaded')\n",
    "\n",
    "events_CF = ['CF-10', 'CF-20', 'CF-50', 'CF-100', 'CF-200', 'CF-500', 'CF-1000']\n",
    "tot_road_CF = pd.read_csv(os.path.join(data_path,'summarized','CF_road_losses.csv'),\n",
    "                          converters = dict(zip(events_CF,[ast.literal_eval]*len(events_CF))),index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Add additional information to the output datasets (protection standards, income group etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_road_FU = tot_road_FU.merge(incomegroups,left_on='country',right_on='CountryCode').merge(global_countries[['ISO_3digit','wbregion']],left_on='country',right_on='ISO_3digit')\n",
    "tot_road_PU = tot_road_PU.merge(incomegroups,left_on='country',right_on='CountryCode').merge(global_countries[['ISO_3digit','wbregion']],left_on='country',right_on='ISO_3digit')\n",
    "tot_road_CF = tot_road_CF.merge(incomegroups,left_on='country',right_on='CountryCode').merge(global_countries[['ISO_3digit','wbregion']],left_on='country',right_on='ISO_3digit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tqdm.pandas()\n",
    "tot_road_FU_base = tot_road_FU.progress_apply(lambda x : set_prot_standard(x,prot_lookup,events_FU),axis=1)\n",
    "tot_road_CF_base = tot_road_CF.progress_apply(lambda x : set_prot_standard(x,prot_lookup,events_CF),axis=1)\n",
    "tot_road_FU_base = tot_road_FU_base.progress_apply(lambda x : pluvial_design(x,'FU'),axis=1)\n",
    "tot_road_CF_base = tot_road_CF_base.progress_apply(lambda x : pluvial_design(x,'CF'),axis=1)\n",
    "tot_road_PU_base = tot_road_PU.progress_apply(lambda x : pluvial_design(x,'PU'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tqdm.pandas()\n",
    "tot_road_FU_1up = tot_road_FU.progress_apply(lambda x : set_prot_standard(x,prot_lookup,events_FU),axis=1)\n",
    "tot_road_CF_1up = tot_road_CF.progress_apply(lambda x : set_prot_standard(x,prot_lookup,events_CF),axis=1)\n",
    "tot_road_FU_1up = tot_road_FU_1up.progress_apply(lambda x : pluvial_design_1up(x,'FU'),axis=1)\n",
    "tot_road_CF_1up = tot_road_CF_1up.progress_apply(lambda x : pluvial_design_1up(x,'CF'),axis=1)\n",
    "tot_road_PU_1up = tot_road_PU.progress_apply(lambda x : pluvial_design_1up(x,'PU'),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Groupby income group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FU_wb_stats = tot_road_FU_base.loc[tot_road_FU.road_type.isin(['primary','secondary','tertiary'])]\n",
    "FU_wb_stats = FU_wb_stats.groupby(['GroupName'])[events_FU].agg(sum_tuples)\n",
    "\n",
    "PU_wb_stats = tot_road_PU_base.loc[tot_road_PU.road_type.isin(['primary','secondary','tertiary'])]\n",
    "PU_wb_stats = PU_wb_stats.groupby(['GroupName'])[events_PU].agg(sum_tuples)\n",
    "\n",
    "CF_wb_stats = tot_road_CF_base.loc[tot_road_CF.road_type.isin(['primary','secondary','tertiary'])]\n",
    "CF_wb_stats = CF_wb_stats.groupby(['GroupName'])[events_CF].agg(sum_tuples)\n",
    "\n",
    "FU_wb_stats_1up = tot_road_FU_1up.loc[tot_road_FU.road_type.isin(['primary','secondary','tertiary'])]\n",
    "FU_wb_stats_1up= FU_wb_stats_1up.groupby(['GroupName'])[events_FU].agg(sum_tuples)\n",
    "\n",
    "PU_wb_stats_1up = tot_road_PU_1up.loc[tot_road_PU.road_type.isin(['primary','secondary','tertiary'])]\n",
    "PU_wb_stats_1up = PU_wb_stats_1up.groupby(['GroupName'])[events_PU].agg(sum_tuples)\n",
    "\n",
    "CF_wb_stats_1up = tot_road_CF_1up.loc[tot_road_CF.road_type.isin(['primary','secondary','tertiary'])]\n",
    "CF_wb_stats_1up = CF_wb_stats_1up.groupby(['GroupName'])[events_CF].agg(sum_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Calculate the flood risk for the baseline and adaptation option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPS = [1/5,1/10,1/20,1/50,1/75,1/100,1/200,1/250,1/500,1/1000]\n",
    "wb_risk_PU = pd.DataFrame(PU_wb_stats.apply(lambda x: calc_risk_total(x,'PU',RPS,events_PU),axis=1).tolist(),index=PU_wb_stats.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "wb_risk_PU_1up = pd.DataFrame(PU_wb_stats_1up.apply(lambda x: calc_risk_total(x,'PU',RPS,events_PU),axis=1).tolist(),index=PU_wb_stats_1up.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "\n",
    "wb_risk_FU = pd.DataFrame(FU_wb_stats.apply(lambda x: calc_risk_total(x,'FU',RPS,events_FU),axis=1).tolist(),index=FU_wb_stats.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "wb_risk_FU_1up = pd.DataFrame(FU_wb_stats_1up.apply(lambda x: calc_risk_total(x,'FU',RPS,events_FU),axis=1).tolist(),index=FU_wb_stats_1up.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "\n",
    "\n",
    "RPS = [1/10,1/20,1/50,1/100,1/200,1/500,1/1000]\n",
    "wb_risk_CF = pd.DataFrame(CF_wb_stats.apply(lambda x: calc_risk_total(x,'CF',RPS,events_CF),axis=1).tolist(),\n",
    "                       index=CF_wb_stats.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "wb_risk_CF_1up = pd.DataFrame(CF_wb_stats_1up.apply(lambda x: calc_risk_total(x,'CF',RPS,events_CF),axis=1).tolist(),\n",
    "                       index=CF_wb_stats.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Groupby region (for the map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FU_reg_stats = tot_road_FU_base.loc[tot_road_FU.road_type.isin(['primary','secondary','tertiary'])]\n",
    "FU_reg_stats = FU_reg_stats.groupby(['region'])[events_FU].agg(sum_tuples)\n",
    "\n",
    "PU_reg_stats = tot_road_PU_base.loc[tot_road_PU.road_type.isin(['primary','secondary','tertiary'])]\n",
    "PU_reg_stats = PU_reg_stats.groupby(['region'])[events_PU].agg(sum_tuples)\n",
    "\n",
    "CF_reg_stats = tot_road_CF_base.loc[tot_road_CF.road_type.isin(['primary','secondary','tertiary'])]\n",
    "CF_reg_stats = CF_reg_stats.groupby(['region'])[events_CF].agg(sum_tuples)\n",
    "\n",
    "FU_reg_stats_1up = tot_road_FU_1up.loc[tot_road_FU.road_type.isin(['primary','secondary','tertiary'])]\n",
    "FU_reg_stats_1up= FU_reg_stats_1up.groupby(['region'])[events_FU].agg(sum_tuples)\n",
    "\n",
    "PU_reg_stats_1up = tot_road_PU_1up.loc[tot_road_PU.road_type.isin(['primary','secondary','tertiary'])]\n",
    "PU_reg_stats_1up = PU_reg_stats_1up.groupby(['region'])[events_PU].agg(sum_tuples)\n",
    "\n",
    "CF_reg_stats_1up = tot_road_CF_1up.loc[tot_road_CF.road_type.isin(['primary','secondary','tertiary'])]\n",
    "CF_reg_stats_1up = CF_reg_stats_1up.groupby(['region'])[events_CF].agg(sum_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Calculate the flood risk for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tqdm.pandas()\n",
    "RPS = [1/5,1/10,1/20,1/50,1/75,1/100,1/200,1/250,1/500,1/1000]\n",
    "reg_risk_PU = pd.DataFrame(PU_reg_stats.progress_apply(lambda x: calc_risk_total(x,'PU',RPS,events_PU),axis=1).tolist(),index=PU_reg_stats.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "reg_risk_PU_1up = pd.DataFrame(PU_reg_stats_1up.progress_apply(lambda x: calc_risk_total(x,'PU',RPS,events_PU),axis=1).tolist(),index=PU_reg_stats_1up.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "\n",
    "reg_risk_FU = pd.DataFrame(FU_reg_stats.progress_apply(lambda x: calc_risk_total(x,'FU',RPS,events_FU),axis=1).tolist(),index=FU_reg_stats.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "reg_risk_FU_1up = pd.DataFrame(FU_reg_stats_1up.progress_apply(lambda x: calc_risk_total(x,'FU',RPS,events_FU),axis=1).tolist(),index=FU_reg_stats_1up.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "\n",
    "\n",
    "RPS = [1/10,1/20,1/50,1/100,1/200,1/500,1/1000]\n",
    "reg_risk_CF = pd.DataFrame(CF_reg_stats.progress_apply(lambda x: calc_risk_total(x,'CF',RPS,events_CF),axis=1).tolist(),\n",
    "                       index=CF_reg_stats.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])\n",
    "reg_risk_CF_1up = pd.DataFrame(CF_reg_stats_1up.progress_apply(lambda x: calc_risk_total(x,'CF',RPS,events_CF),axis=1).tolist(),\n",
    "                       index=CF_reg_stats.index,\n",
    "     columns=['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Estimate the absolute and relative reduction in risk due to the increase in design standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adapt_rel = pd.concat([pd.DataFrame(np.array((wb_risk_CF_1up-wb_risk_CF)/wb_risk_CF),columns=pd.MultiIndex.from_product([['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'], ['Coastal Flooding']])),\n",
    "pd.DataFrame(np.array((wb_risk_PU_1up-wb_risk_PU)/wb_risk_PU),columns=pd.MultiIndex.from_product([['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'], ['Surface Flooding']])),\n",
    "pd.DataFrame(np.array((wb_risk_FU_1up-wb_risk_FU)/wb_risk_FU),columns=pd.MultiIndex.from_product([['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'], ['River Flooding']]))],axis=1)\n",
    "df_adapt_rel = df_adapt_rel*-1\n",
    "df_adapt_rel.index = ['High\\nIncome','Low\\nIncome','Lower Middle\\nIncome','Upper Middle\\nIncome']\n",
    "\n",
    "df_adapt_abs = pd.concat([pd.DataFrame(np.array((wb_risk_CF_1up-wb_risk_CF)),columns=pd.MultiIndex.from_product([['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'], ['Coastal Flooding']])),\n",
    "pd.DataFrame(np.array((wb_risk_PU_1up-wb_risk_PU)),columns=pd.MultiIndex.from_product([['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'], ['Surface Flooding']])),\n",
    "pd.DataFrame(np.array((wb_risk_FU_1up-wb_risk_FU)),columns=pd.MultiIndex.from_product([['perc_0','perc_20','perc_40','perc_50','perc_60','perc_80','perc_100'], ['River Flooding']]))],axis=1)\n",
    "\n",
    "df_adapt_abs = df_adapt_abs/-1000000\n",
    "df_adapt_abs.index = ['High\\nIncome','Low\\nIncome','Lower Middle\\nIncome','Upper Middle\\nIncome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.concat([pd.DataFrame(reg_risk_CF.mean(axis=1),columns=['Coastal Flooding']),\n",
    "pd.DataFrame(reg_risk_PU.mean(axis=1),columns=['Surface Flooding']),\n",
    "pd.DataFrame(reg_risk_FU.mean(axis=1),columns=['River Flooding']),\n",
    "pd.DataFrame(reg_risk_CF_1up.mean(axis=1),columns=['Coastal Flooding Up']),\n",
    "pd.DataFrame(reg_risk_PU_1up.mean(axis=1),columns=['Surface Flooding Up']),\n",
    "pd.DataFrame(reg_risk_FU_1up.mean(axis=1),columns=['River Flooding Up'])],axis=1,sort=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg['baseline_all'] = df_reg['Coastal Flooding']+df_reg['Surface Flooding']+df_reg['River Flooding']\n",
    "df_reg['1up_all'] = df_reg['Coastal Flooding Up']+df_reg['Surface Flooding Up']+df_reg['River Flooding Up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg['abs_increase'] = (df_reg['1up_all'] - df_reg['baseline_all'])*-1 \n",
    "df_reg['rel_increase'] =((df_reg['1up_all'] - df_reg['baseline_all']) /  df_reg['baseline_all'])*-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_adapt = global_regions.merge(df_reg,left_on='GID_2',right_index=True,how='outer').fillna(0)\n",
    "global_adapt = global_adapt.loc[~(global_adapt.geometry == 0)]\n",
    "global_adapt['geometry'] = global_adapt.buffer(0.075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adapt_abs = pd.concat([df_adapt_abs.iloc[1:,:], pd.DataFrame(df_adapt_abs.iloc[0,:]).T])\n",
    "df_adapt_rel = pd.concat([df_adapt_rel.iloc[1:,:], pd.DataFrame(df_adapt_rel.iloc[0,:]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(((wb_risk_CF_1up.sum()+wb_risk_PU_1up.sum()+wb_risk_FU_1up.sum())-(wb_risk_CF.sum()+wb_risk_PU.sum()+wb_risk_FU.sum()))/(wb_risk_CF.sum()+wb_risk_PU.sum()+wb_risk_FU.sum())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(dict(zip(['CF','PU','FU'],[wb_risk_CF,wb_risk_PU,wb_risk_FU]))).to_csv(os.path.join(data_path,'output_data','wb_income_prot_base.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Create Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,18))\n",
    "\n",
    "gs = gridspec.GridSpec(3, 2)\n",
    "gs.update(left=0.0, right=1, wspace=0.1,hspace=-0.01)\n",
    "\n",
    "ax_gdp = plt.subplot(gs[:-1, :])\n",
    "labels = [0,1,2,3,4,5]\n",
    "label_names = ['N/A','0-20%','20-40%','40-60%','60-80%','80-100%']\n",
    "color_scheme =  ['#F6F6F7','#F2D7EE','#D3BCC0','#A5668B','#69306D','#0E103D']\n",
    "flatui = ['#E9C46A','#DB9157','#E76F51']\n",
    "letters = ['A','B','C']\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(name='continents',\n",
    "                                     colors=color_scheme)  \n",
    "\n",
    "bins = [0,1,20,40,60,80,100]\n",
    "\n",
    "coastlines = gpd.read_file(os.path.join(data_path,'input_data','ne_10m_coastline.shp'))\n",
    "coastlines = coastlines.cx[-180:180, -50:90]\n",
    "coastlines.plot(ax=ax_gdp,facecolor = 'none',edgecolor='black',linewidth=0.1)\n",
    "global_adapt['rel_bin'] = pd.cut(global_adapt['rel_increase'], bins=bins, labels=labels).fillna(0)\n",
    "\n",
    "global_adapt.plot(column='rel_bin',ax=ax_gdp,cmap=cmap,linewidth=0.0)\n",
    "\n",
    "legend_elements = [Patch(facecolor=color_scheme[0],label=label_names[0]),\n",
    "                  Patch(facecolor=color_scheme[1],label=label_names[1]),\n",
    "                  Patch(facecolor=color_scheme[2],label=label_names[2]),\n",
    "                  Patch(facecolor=color_scheme[3],label=label_names[3]),\n",
    "                  Patch(facecolor=color_scheme[4],label=label_names[4]),\n",
    "                  Patch(facecolor=color_scheme[5],label=label_names[5])]        \n",
    "\n",
    "legend = ax_gdp.legend(handles=legend_elements, shadow=True, \n",
    "                   fancybox=True,facecolor='#fefdfd',prop={'size':21},loc='lower left') #\n",
    "\n",
    "ax_gdp.set_xticks([])\n",
    "ax_gdp.set_yticks([])\n",
    "ax_gdp.patch.set_facecolor('white')\n",
    "\n",
    "ax_gdp.text(0.5, 0.99, '{}) Relative reduction in risk per region'.format(letters[0]), transform=ax_gdp.transAxes,\n",
    "            fontweight=\"bold\",color='black', fontsize=20,    verticalalignment='top',horizontalalignment='center', \n",
    "            bbox= dict(boxstyle='round', facecolor='white', alpha=0.5,linewidth=0))\n",
    "for iter_ in range(2):\n",
    "    if iter_ == 0:\n",
    "        ax = plt.subplot(gs[-1, :-1])\n",
    "\n",
    "        df = pd.DataFrame(df_adapt_abs.unstack(1),columns=['Value']).reset_index()\n",
    "        sns.boxplot(x=df.level_2,y=df.Value,hue=df.level_1,data=df,ax=ax,palette = sns.color_palette(flatui),showfliers=False)\n",
    "        \n",
    "        ax.yaxis.set_ticks(np.arange(0,3001,600))\n",
    "        ax.set_xlabel(xlabel='Income Group',fontweight=\"bold\",color='black',fontsize=20) \n",
    "        ax.set_ylabel(ylabel='Million Dollar',fontweight=\"bold\",color='black',fontsize=19) #\n",
    "        legend_elements = [Patch(facecolor=flatui[0],edgecolor='black',linewidth=0.3,label='Coastal Flooding'),\n",
    "                           Patch(facecolor=flatui[1],edgecolor='black',linewidth=0.3,label='Surface Flooding'),\n",
    "                           Patch(facecolor=flatui[2],edgecolor='black',linewidth=0.3,label='River Flooding')]        \n",
    "\n",
    "        legend = ax.legend(handles=legend_elements, shadow=True, \n",
    "                                   fancybox=True,facecolor='#fefdfd',prop={'size':18.5},loc=2)    \n",
    "        \n",
    "        ax.set_title('{}) Absolute Reduction'.format(letters[iter_+1]),fontweight='bold',fontsize=20)\n",
    "\n",
    "    if iter_ == 1:\n",
    "        ax = plt.subplot(gs[-1, -1])\n",
    "\n",
    "        df = pd.DataFrame(df_adapt_rel.unstack(1)*100 ,columns=['Value']).reset_index()\n",
    "        sns.boxplot(x=df.level_2,y=df.Value,hue=df.level_1,data=df,ax=ax,palette = sns.color_palette(flatui),showfliers=False)\n",
    "        \n",
    "        ax.yaxis.set_ticks(np.arange(0,101,20))\n",
    "        ax.set_xlabel(xlabel='Income Group',fontweight=\"bold\",color='black',fontsize=20) \n",
    "        ax.set_ylabel(ylabel='%',fontweight=\"bold\",color='black',fontsize=19) #\n",
    "        ax.legend_.remove()\n",
    "        ax.set_title('{}) Relative Reduction'.format(letters[iter_+1]),fontweight='bold',fontsize=20)\n",
    "        \n",
    "    ax.tick_params(axis = 'both',labelcolor='black',color='black',labelsize=18) #\n",
    "    ax.set_facecolor('#f6f1f1')\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.get_xaxis().set_label_coords(0.5,-0.2)\n",
    "    ax.set_facecolor('#FAF9F9')\n",
    "\n",
    "plt.savefig(os.path.join(figure_path,'Fig6_adaptation.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
