{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook to create Figure 1 and Figure 2 of Koks et al. (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: load all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import YearLocator\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.patches\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager\n",
    "mpl.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from itertools import groupby\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('chained_assignment',None)\n",
    "\n",
    "sys.path.append(os.path.join( '..'))\n",
    "from miriam_py.utils import load_config\n",
    "data_path = load_config()['paths']['data']\n",
    "figure_path = load_config()['paths']['figures']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load datasets with additional information about countries and regions to create the maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "global_countries = gpd.read_file(os.path.join(data_path,'input_data','global_countries.shp'))\n",
    "global_regions = gpd.read_file(os.path.join(data_path,'input_data','global_regions_v2.shp'))\n",
    "incomegroups = pd.read_csv(os.path.join(data_path,'input_data','incomegroups_2018.csv'),index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load road exposure data and other road asset specific stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_road = pd.read_csv(os.path.join(data_path,'summarized','total_exposure_100prot.csv'))\n",
    "tot_road = tot_road.merge(incomegroups,left_on='country',right_on='CountryCode')\n",
    "tot_road = tot_road.loc[tot_road.GroupCode.isin(['LIC','HIC','UMC','LMC'])]\n",
    "tot_road = tot_road.loc[tot_road.road_type.isin(['primary','secondary','tertiary'])]\n",
    "\n",
    "all_road_stats = pd.read_feather(os.path.join(data_path,'summarized','all_road_stats.ft'))\n",
    "all_road_stats = pd.DataFrame(all_road_stats.groupby('region').sum().max(axis=1),columns=['all_risk_road'])\n",
    "\n",
    "# only keep the asset which are most at risk\n",
    "tot_road_inb = tot_road.drop([x for x in tot_road.columns if ('risk_1' in x) | ('risk_2' in x)],axis='columns')\n",
    "region_risk_road = pd.DataFrame(tot_road_inb.groupby('region').sum().sum(axis=1),columns=['tot_risk_road'])\n",
    "region_risk_road = region_risk_road.merge(all_road_stats,left_index=True,right_index=True)\n",
    "region_risk_road['rel_risk'] = (region_risk_road.tot_risk_road/region_risk_road.all_risk_road)*100\n",
    "global_road = global_regions.merge(region_risk_road,left_on='GID_2',right_index=True,how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load road exposure data and other road asset specific stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_rail = pd.read_csv(os.path.join(data_path,'summarized','total_exposure_100prot_rail.csv'))\n",
    "tot_rail = tot_rail.merge(incomegroups,left_on='country',right_on='CountryCode')\n",
    "tot_rail = tot_rail.loc[tot_rail.GroupCode.isin(['LIC','HIC','UMC','LMC'])]\n",
    "# list of railway segments to remove\n",
    "rail_to_remove =['disused','abandoned','dismantled','preserved', 'proposed','razed', \n",
    "                 'planned','no','historical','na','not_built','abandonned', 'uncompleted', 'demolished',\n",
    "                 'abandoned_tram','construction;rail', 'rail;construction','waste_disposal','collapsed']\n",
    "\n",
    "tot_rail = tot_rail.loc[~(tot_rail.infra_type.isin(rail_to_remove))]\n",
    "\n",
    "all_rail_stats = pd.read_feather(os.path.join(data_path,'summarized','all_railway_stats.ft'))\n",
    "all_rail_stats = pd.DataFrame(all_rail_stats.groupby('region').sum().max(axis=1),columns=['all_risk_rail'])\n",
    "\n",
    "# only keep the asset which are most at risk\n",
    "tot_rail_inb = tot_rail.drop([x for x in tot_rail.columns if ('risk_1' in x) | ('risk_2' in x)],axis='columns')\n",
    "region_risk_rail = pd.DataFrame(tot_rail_inb.groupby('region').sum().sum(axis=1),columns=['tot_risk_rail'])\n",
    "region_risk_rail = region_risk_rail.merge(all_rail_stats,left_index=True,right_index=True)\n",
    "region_risk_rail['rel_risk'] = (region_risk_rail.tot_risk_rail/region_risk_rail.all_risk_rail)*100\n",
    "global_rail = global_regions.merge(region_risk_rail,left_on='GID_2',right_index=True,how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Group the data into the four income level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "global_infra = global_road.copy()\n",
    "global_infra['tot_risk'] = global_road['tot_risk_road'] + global_rail['tot_risk_rail']\n",
    "global_infra['all_risk'] = global_road['all_risk_road'] + global_rail['all_risk_rail']\n",
    "global_infra['rel_risk'] = (global_infra.tot_risk/global_infra.all_risk)*100\n",
    "global_infra = global_infra.loc[~(global_infra.geometry == 0)]\n",
    "global_infra['geometry'] = global_infra.buffer(0.075)\n",
    "summed_all = tot_road.groupby('GroupCode').sum() + tot_rail.groupby('GroupCode').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,18))\n",
    "\n",
    "gs = gridspec.GridSpec(3, 5)\n",
    "gs.update(left=0.0, right=1, wspace=0.1,hspace=-0.01)\n",
    "\n",
    "ax_gdp = plt.subplot(gs[:-1, :])\n",
    "labels = [0,1,2,3,4]\n",
    "label_names = ['Very Low','Low','Medium','High','Very high']\n",
    "color_scheme_map =  ['#fee5d9','#fcae91','#fb6a4a','#de2d26','#a50f15'] # ['#feedde','#fdbe85','#fd8d3c','#e6550d','#a63603']\n",
    "letters = ['A','B','C','D','E','F']\n",
    "\n",
    "\n",
    "color_scheme = ['#F3FFBD','#B2DBBF','#70C1B3','#247BA0'] #['#bae4bc','#7bccc4','#43a2ca','#0868ac']\n",
    "income_dict = ['LIC', 'LMC', 'UMC', 'HIC']\n",
    "color_lookup = dict(zip(income_dict,color_scheme))\n",
    "\n",
    "hazards = ['Cyc','EQ','PU','FU','CF']\n",
    "hazards_full = ['Cyclones','Earthquakes','Surface Flooding','River Flooding','Coastal Flooding']\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(name='continents',\n",
    "                                     colors=color_scheme_map)  \n",
    "\n",
    "coastlines = gpd.read_file(os.path.join(data_path,'input_data','ne_10m_coastline.shp'))\n",
    "coastlines = coastlines.cx[-180:180, -50:90]\n",
    "coastlines.plot(ax=ax_gdp,facecolor = 'none',edgecolor='black',linewidth=0.1)\n",
    "\n",
    "bins = [-1,global_infra.rel_risk.quantile(0.2),global_infra.rel_risk.quantile(0.4),global_infra.rel_risk.quantile(0.6),\n",
    "         global_infra.rel_risk.quantile(0.8),global_infra.rel_risk.quantile(1)]\n",
    "global_infra['risk_gdp_bin'] = pd.cut(global_infra['rel_risk'], bins=bins, labels=labels).fillna(0)\n",
    "global_infra.plot(column='risk_gdp_bin',ax=ax_gdp,cmap=cmap,linewidth=0)\n",
    "\n",
    "\n",
    "legend_elements = [Patch(facecolor=color_scheme_map[0],label=label_names[0]),\n",
    "                  Patch(facecolor=color_scheme_map[1],label=label_names[1]),\n",
    "                  Patch(facecolor=color_scheme_map[2],label=label_names[2]),\n",
    "                  Patch(facecolor=color_scheme_map[3],label=label_names[3]),\n",
    "                  Patch(facecolor=color_scheme_map[4],label=label_names[4])]        \n",
    "\n",
    "legend = ax_gdp.legend(handles=legend_elements, shadow=True, \n",
    "                   fancybox=True,facecolor='#fefdfd',prop={'size':22},loc='lower left')\n",
    "\n",
    "ax_gdp.text(0.5, 0.99, '{}) Relative multi-hazard exposure per region'.format(letters[0]), transform=ax_gdp.transAxes,\n",
    "            fontweight=\"bold\",color='black', fontsize=20,    verticalalignment='top',horizontalalignment='center', \n",
    "            bbox= dict(boxstyle='round', facecolor='white', alpha=0.5,linewidth=0))\n",
    "\n",
    "ax_gdp.set_xticks([])\n",
    "ax_gdp.set_yticks([])\n",
    "\n",
    "ax_gdp.patch.set_facecolor('white')\n",
    "\n",
    "for iter_ in range(5):\n",
    "    ax_sub = plt.subplot(gs[-1, iter_])\n",
    "    \n",
    "    hazard = hazards[iter_]\n",
    "    sub_summed = summed_all[[x for x in summed_all.columns if (hazard in x)]]\n",
    "    \n",
    "    if hazard == 'EQ':\n",
    "        sub_summed.columns = ['0.092g-0.18g','0.18g-0.34g','0.34g-0.65g','>0.65g']\n",
    "        sub_summed['color'] = [color_lookup[x] for x in list(sub_summed.index)]\n",
    "        sub_summed = sub_summed.sort_values(by='0.092g-0.18g',ascending=False)\n",
    "        sorted_colors = list(sub_summed['color'])\n",
    "        sub_summed = sub_summed.drop('color',axis=1)\n",
    "    elif  hazard == 'Cyc':\n",
    "        sub_summed.columns = ['154-178/h','178-209/h','209-252/h','>252/h']\n",
    "        sub_summed['color'] = [color_lookup[x] for x in list(sub_summed.index)]\n",
    "        sub_summed = sub_summed.sort_values(by='154-178/h',ascending=False)\n",
    "        sorted_colors = list(sub_summed['color'])\n",
    "        sub_summed = sub_summed.drop('color',axis=1)\n",
    "    elif  (hazard == 'FU') | (hazard == 'PU') | (hazard == 'CF'):\n",
    "        sub_summed.columns = ['25-50cm','50cm-1m','1m-2m','>2m']\n",
    "        sub_summed['color'] = [color_lookup[x] for x in list(sub_summed.index)]\n",
    "        sub_summed = sub_summed.sort_values(by='25-50cm',ascending=False)\n",
    "        sorted_colors = list(sub_summed['color'])\n",
    "        sub_summed = sub_summed.drop('color',axis=1)\n",
    "\n",
    "    sub_summed = sub_summed/1000\n",
    "    sub_summed.index.name = None\n",
    "    cmap = LinearSegmentedColormap.from_list(name='continents',\n",
    "                                         colors=sorted_colors)    \n",
    "    if iter_ == 4:\n",
    "        sub_summed.T.plot(kind='bar',stacked=True,ax=ax_sub,cmap=cmap,edgecolor='black',linewidth=0.5)\n",
    "        legend_elements = [Patch(facecolor=color_scheme[0],edgecolor='black',linewidth=0.3,label='Low income'),\n",
    "                          Patch(facecolor=color_scheme[1],edgecolor='black',linewidth=0.3,label='Lower middle income'),\n",
    "                          Patch(facecolor=color_scheme[2],edgecolor='black',linewidth=0.3,label='Upper middle income'),\n",
    "                          Patch(facecolor=color_scheme[3],edgecolor='black',linewidth=0.3,label='High income')]        \n",
    "\n",
    "        legend = ax_sub.legend(handles=legend_elements, shadow=True, \n",
    "                           fancybox=True,facecolor='#fefdfd',prop={'size':15})\n",
    "          \n",
    "    else:\n",
    "        sub_summed.T.plot(kind='bar',stacked=True,ax=ax_sub,cmap=cmap,legend=False,linewidth=0.5,edgecolor='black')\n",
    "        \n",
    "    if iter_ < 5:\n",
    "        ax_sub.set_title('{}) {}'.format(letters[iter_+1],hazards_full[iter_]),fontweight='bold',fontsize=19)\n",
    "        ax_sub.yaxis.set_major_formatter(ScalarFormatter())\n",
    "\n",
    "    if iter_ == 0:\n",
    "        ax_sub.set_ylabel('Annual exposed km (x1000)',fontweight='bold',color='black',fontsize=17)\n",
    "        \n",
    "    for tick in ax_sub.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "\n",
    "    ax_sub.tick_params(axis = 'both',labelsize=16,labelcolor='black',color='black')\n",
    "\n",
    "    ax_sub.get_xaxis().set_label_coords(0.5,-0.25)\n",
    "    ax_sub.get_yaxis().set_label_coords(-.15,.5)\n",
    "    \n",
    "    yaxis_allticks = [np.arange(0,40.1,8.),np.arange(0,30.1,6),np.arange(0,120.1,24),np.arange(0,20.1,4),np.arange(0,6.1,1.2)]\n",
    "    \n",
    "    ax_sub.yaxis.set_ticks(yaxis_allticks[iter_])\n",
    "    ax_sub.set_ylim(top=yaxis_allticks[iter_][-1])\n",
    "    \n",
    "    ax_sub.set_facecolor('#FAF9F9')\n",
    "    ax_sub.spines['left'].set_color('black')\n",
    "    ax_sub.spines['bottom'].set_color('black')\n",
    "    \n",
    "    if hazard == 'EQ':\n",
    "        ax_sub.set_xlabel(xlabel='PGA',fontweight=\"bold\",color='black',fontsize=17)\n",
    "    elif hazard == 'Cyc':\n",
    "        ax_sub.set_xlabel(xlabel='Wind Speed',fontweight=\"bold\",color='black',fontsize=17)\n",
    "    else:\n",
    "        ax_sub.set_xlabel(xlabel='Water Depth',fontweight=\"bold\",color='black',fontsize=17)\n",
    "        \n",
    "plt.savefig(os.path.join(figure_path,'Fig1_exposure.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Identify the most dominant hazard (i.e. hazard with the highest relative exposure) per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tot_dom_hz = pd.DataFrame(tot_road_inb.groupby('region').sum()) #[['risk_4_Cyc','risk_4_EQ','risk_4_PU','risk_4_FU','risk_4_CF']]\n",
    "tot_dom_hz = tot_dom_hz.loc[~(tot_dom_hz==0).all(axis=1)]\n",
    "tot_dom_hz = pd.DataFrame(tot_dom_hz.idxmax(axis=1),columns=['dom_hz'])\n",
    "tot_dom_hz['dom_hz'] = tot_dom_hz['dom_hz'].apply(lambda x: x.split('_')[2])\n",
    "global_dom_hz = global_infra.merge(tot_dom_hz,\n",
    "                                     left_on='GID_2',right_index=True,how='outer')\n",
    "global_dom_hz['dom_hz'] = global_dom_hz.dom_hz.fillna('NA')\n",
    "global_dom_hz['dom_hz'].loc[global_dom_hz['all_risk_road'] < 10] = 'NA'\n",
    "global_dom_hz = global_dom_hz.loc[~(global_dom_hz.geometry == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intdict = dict(zip(['NA','EQ','Cyc','PU','FU','CF'],[0,1,2,3,4,5]))\n",
    "global_dom_hz['int_dom_hz'] = global_dom_hz.dom_hz.apply(lambda x : intdict[x])\n",
    "global_dom_hz['int_dom_hz'].loc[(global_dom_hz.int_dom_hz == 0) & (global_dom_hz.GID_0.isin(['SDN','AUS','SAU','CHN','CAN','BRA','PER']))] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ctry_exp['rel_risk'] = ctry_exp['tot_risk'].div(ctry_exp['all_risk'])\n",
    "ctry_exp = tot_road.groupby('country').sum()\n",
    "ctry_exp['EQ'] = (ctry_exp['risk_3_EQ']+ctry_exp['risk_4_EQ']).div(tot_road.groupby('country').sum().sum(axis=1))\n",
    "ctry_exp['Cyc'] = (ctry_exp['risk_3_Cyc']+ctry_exp['risk_4_Cyc']).div(tot_road.groupby('country').sum().sum(axis=1))\n",
    "ctry_exp['PU'] = (ctry_exp['risk_3_PU']+ctry_exp['risk_4_PU']).div(tot_road.groupby('country').sum().sum(axis=1))\n",
    "ctry_exp['FU'] = (ctry_exp['risk_3_FU']+ctry_exp['risk_4_FU']).div(tot_road.groupby('country').sum().sum(axis=1))\n",
    "ctry_exp['CF'] = (ctry_exp['risk_3_CF']+ctry_exp['risk_4_CF']).div(tot_road.groupby('country').sum().sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dom_hz['areadeg'] = global_dom_hz.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(global_dom_hz.groupby('dom_hz')['areadeg'].sum())\n",
    "\n",
    "df.columns=['val1']\n",
    "df = df.reindex(['NA', 'EQ', 'Cyc','PU','FU','CF'])\n",
    "df = df[1:]\n",
    "df['cum_sum'] = df.val1.cumsum()\n",
    "df['cum_perc'] = df.cum_sum/df.val1.sum()\n",
    "df['theta'] = df.cum_perc*360\n",
    "df['perc'] = df.val1/df.val1.sum()*100\n",
    "thetas = list(df['theta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1,figsize=(20,12))\n",
    "color_scheme_map = ['#ededed','#264653','#2A9D8F','#E9C46A','#DB9157','#E76F51'] \n",
    "hazards_full = ['No Data','Earthquakes','Cyclones','Surface Flooding','River Flooding','Coastal Flooding']\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(name='continents',\n",
    "                                     colors=color_scheme_map)  \n",
    "\n",
    "coastlines = gpd.read_file(os.path.join(data_path,'input_data','ne_10m_coastline.shp'))\n",
    "coastlines = coastlines.cx[-180:180, -50:90]\n",
    "coastlines.plot(ax=ax_gdp,facecolor = 'none',edgecolor='black',linewidth=0.1)\n",
    "\n",
    "global_dom_hz.plot(column='int_dom_hz',ax=ax,linewidth=0,cmap=cmap)\n",
    "\n",
    "legend_elements = [Patch(facecolor=color_scheme_map[0],label=hazards_full[0]),\n",
    "                  Patch(facecolor=color_scheme_map[1],label=hazards_full[1]),\n",
    "                  Patch(facecolor=color_scheme_map[2],label=hazards_full[2]),\n",
    "                  Patch(facecolor=color_scheme_map[3],label=hazards_full[3]),\n",
    "                   Patch(facecolor=color_scheme_map[4],label=hazards_full[4]),\n",
    "                  Patch(facecolor=color_scheme_map[5],label=hazards_full[5])]        \n",
    "\n",
    "legend = ax.legend(handles=legend_elements,edgecolor='#fefdfd',facecolor='#fefdfd',prop={'size':16},loc='lower left') #, shadow=True, fancybox=True,\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "wed1 = matplotlib.patches.Wedge((-161, 15), 20, 0, thetas[0],facecolor=color_scheme_map[1],linewidth=1,edgecolor='white')\n",
    "wed2 = matplotlib.patches.Wedge((-161, 15), 20, thetas[0], thetas[1],facecolor=color_scheme_map[2],linewidth=1,edgecolor='white')\n",
    "wed3 = matplotlib.patches.Wedge((-161, 15), 20, thetas[1], thetas[2],facecolor=color_scheme_map[3],linewidth=1,edgecolor='white')\n",
    "wed4 = matplotlib.patches.Wedge((-161, 15), 20, thetas[2], thetas[3],facecolor=color_scheme_map[4],linewidth=1,edgecolor='white')\n",
    "wed5 = matplotlib.patches.Wedge((-161, 15), 20, thetas[3], thetas[4],facecolor=color_scheme_map[5],linewidth=1,edgecolor='white')\n",
    "\n",
    "# set translation \n",
    "dx, dy = 5/72., -5/72.\n",
    "offset = transforms.ScaledTranslation(dx, dy, fig.dpi_scale_trans)\n",
    "shadow_transform = ax.transData + offset\n",
    "\n",
    "# plot patch shadow\n",
    "rec1 =  matplotlib.patches.FancyBboxPatch((-195,-65), 65, 105, transform=shadow_transform, \n",
    "                         edgecolor = None, boxstyle='round', facecolor = \"#e4e4e4\")\n",
    "rec2 = matplotlib.patches.FancyBboxPatch((-195,-65), 65, 105, boxstyle='round',facecolor='#fefdfd')\n",
    "\n",
    "ax.add_patch(rec1)\n",
    "ax.add_patch(rec2)\n",
    "\n",
    "ax.add_patch(wed1)\n",
    "ax.add_patch(wed2)\n",
    "ax.add_patch(wed3)\n",
    "ax.add_patch(wed4)\n",
    "ax.add_patch(wed5)\n",
    "\n",
    "ax.annotate(\"8%\", xy=(-140,18), fontsize=15,fontweight='bold')\n",
    "ax.annotate(\"8%\", xy=(-144,28), fontsize=15,fontweight='bold')\n",
    "ax.annotate(\"39%\", xy=(-182,32), fontsize=15,fontweight='bold')\n",
    "ax.annotate(\"34%\", xy=(-171,-10), fontsize=15,fontweight='bold')\n",
    "ax.annotate(\"11%\", xy=(-141,6), fontsize=15,fontweight='bold')\n",
    "\n",
    "ax.patch.set_facecolor('white')\n",
    "plt.savefig(os.path.join(figure_path,'Fig2_dom_hazard.png'), bbox_inches='tight',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
